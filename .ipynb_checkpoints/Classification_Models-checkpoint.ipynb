{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#Evalaluation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Grid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Performance\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit(model, name):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "\n",
    "    df = pd.DataFrame({'PassengerId':test_df.PassengerId, \n",
    "                       'Survived':submission})\n",
    "\n",
    "    print(len(df))\n",
    "    df.to_csv(name,header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Dogs_vs._Cats/train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 8), (891,), (418, 8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('Titanic/clean_train.csv')\n",
    "test_df = pd.read_csv('Titanic/clean_test.csv')\n",
    "\n",
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "oosample  = test_df.drop(\"PassengerId\", axis=1).copy()\n",
    "X.shape, y.shape, oosample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((668, 8), (668,), (223, 8), (223,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use train/test split with different random_state values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      "Pclass       891 non-null int64\n",
      "Sex          891 non-null int64\n",
      "Age          891 non-null int64\n",
      "Fare         891 non-null int64\n",
      "Embarked     891 non-null int64\n",
      "Title        891 non-null float64\n",
      "IsAlone      891 non-null int64\n",
      "Age*Class    891 non-null int64\n",
      "dtypes: float64(1), int64(7)\n",
      "memory usage: 55.8 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "#sns.pairplot(train_df, hue='Survived', size=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classification\n",
    "Probabilistically determine the label for a new point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.8878923767\n",
      "72.5175632732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred= model.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred)*100)\n",
    "print(cross_val_score(model, X, y, cv=10, scoring='accuracy').mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.475229826353427"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model= LogisticRegression()\n",
    "\n",
    "#Fit Model\n",
    "scores= cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "scores.mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esemble Method\n",
    "## Random Forest\n",
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "HyperParameters:\n",
    "- max_features: limits number of number of features in a tree\n",
    "- n_estimators: # of trees built before average prediciton is made\n",
    "- min_sample_leaf: End node of trees. Too small = more noise. For Regression tree.\n",
    "- n_jobs: computer processors utilized. -1 = no restrictions\n",
    "- random_state: seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.0426487093\n",
      "{'n_estimators': 120, 'max_samples': 0.5, 'max_features': 0.6}\n",
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=0.6,\n",
      "         max_samples=0.5, n_estimators=120, n_jobs=1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)\n",
      "peak memory: 136.58 MiB, increment: 2.20 MiB\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# titanic #param_grid ={'max_features': [.2,.3,.4, .5,.6,.8, 1],\n",
    "             'n_estimators': [40,50,60,80,100,120,150,180],\n",
    "             'max_samples': [0.2,0.3,.4,.5,.6]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "#bag = BaggingClassifier(tree)\n",
    "\n",
    "\n",
    "grid = GridSearchCV(BaggingClassifier(tree, n_jobs=1), param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(X, y);\n",
    "print(grid.best_score_*100)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "# model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "#submit(grid.best_estimator_, \"82bagger.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.98\n",
      "80.8173306095\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=500)\n",
    "random_forest.fit(X_train, y_train)\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n",
    "print(acc_random_forest)\n",
    "print(cross_val_score(random_forest, X, y, cv=10, scoring='accuracy').mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0445749631\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=300, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "\n",
    "print(cross_val_score(bag, X, y, cv=10, scoring='accuracy').mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768971456134\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_range = list(range(1, 31))\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "df = pd.DataFrame(k_scores, index=k_range, columns=[\"Scores\"])\n",
    "df['Scores'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Classification\n",
    "Model new points by seeing where it falls upon a divide.\n",
    "Fast prediction phase, work well in high dimensional data, versatile\n",
    "\n",
    "Costly at high quantities of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC\n",
    "Creates a linear divide between point to classify. Maximizes the distance of the discriminatory margin.\n",
    "\n",
    "Hyperparameters:\n",
    "- C= Hardness of the margin. Higher C, less softening.\n",
    "- Gamma= Kernel coefficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.4714561344\n",
      "418\n",
      "peak memory: 121.12 MiB, increment: 0.00 MiB\n",
      "Wall time: 980 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# Define Model\n",
    "model = svm.LinearSVC()\n",
    "#Fit Model\n",
    "scores= cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "print(scores.mean()*100)\n",
    "\n",
    "#submit(svm.LinearSVC(), name=\"80linear_svc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.2 s\n",
      "0.8092031425364759\n",
      "{'kernel': 'rbf', 'C': 1750, 'gamma': 0.001}\n",
      "SVC(C=1750, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "param_grid = [\n",
    "  {'C': [1000, 1500], 'kernel': ['linear']},\n",
    "  {'C': [1000, 1250, 1500], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "grid.fit(X, y);\n",
    "print(grid.best_score_*100)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "# model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + SVC Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.9 s\n",
      "0.792368125701459\n",
      "{'svc__gamma': 0.0005, 'svc__C': 5}\n",
      "Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=6, random_state=42,\n",
      "  svd_solver='randomized', tol=0.0, whiten=True)), ('svc', SVC(C=5, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0005, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(n_components=6, whiten=True, random_state=42, svd_solver='randomized')\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)\n",
    "# pipeline!\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = [{'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]},\n",
    "              {'svc__C': [0.001, 0.01, 0.1, 1, 10, 50, 100, 150, 1000, 1500], 'svc__kernel': ['linear']}]\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "\n",
    "%time grid.fit(X, y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
