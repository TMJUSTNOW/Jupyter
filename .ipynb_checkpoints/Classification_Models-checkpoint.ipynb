{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Evalaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Grid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Performance\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.chdir(r\"D:/My Computer/\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Number Recognition\n",
    "train_df = pd.read_csv(\"Number_Recognition/train.csv\")\n",
    "oosample = pd.read_csv(\"Number_Recognition/test.csv\")\n",
    "\n",
    "#train_df = train_df.take(np.random.permutation(len(train_df))[:2000])\n",
    "\n",
    "X = train_df.iloc[:,1:]\n",
    "y = train_df.iloc[:,0]\n",
    "print(X.shape, y.shape, oosample.shape)\n",
    "\n",
    "def submit(model, name):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "\n",
    "    df = pd.DataFrame({'ImageId':list(range(1, len(submission)+1)), \n",
    "                       'Label':submission})\n",
    "\n",
    "    print(len(df))\n",
    "    df.to_csv(name,header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 8) (891,) (418, 8)\n"
     ]
    }
   ],
   "source": [
    "#Titanic\n",
    "path = r\"C:/Users/Nicol/Google Drive/Learning/Jupyter/Titanic\"\n",
    "train_df = pd.read_csv(open(os.path.join(path, \"clean_train.csv\"), \"r\")) \n",
    "test_df = pd.read_csv(open(os.path.join(path, \"clean_test.csv\"), \"r\")) \n",
    "\n",
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "\n",
    "oosample  = test_df.drop([\"PassengerId\"] , axis=1).copy()\n",
    "print(X.shape, y.shape, oosample.shape)\n",
    "\n",
    "results=[]\n",
    "def save(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.PassengerId, \n",
    "                           'Survived':submission})\n",
    "    print(len(df))\n",
    "    df.to_csv((\"Titanic/submissions/{}.csv\".format(modelname)),header=True,index=False)\n",
    "    \n",
    "    # CV and Save Scores\n",
    "    trainingscore = (grid.best_score_*100)\n",
    "    results.append(((trainingscore),(\"{}\".format(modelname)), grid.best_params_))\n",
    "    print(trainingscore)\n",
    "    print(grid.best_params_)\n",
    "\n",
    "    with open((r\"Titanic/Pickle/{}.pickle\".format(modelname)), 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((668, 8), (668,), (223, 8), (223,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use train/test split with different random_state values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      "Pclass       891 non-null int64\n",
      "Sex          891 non-null int64\n",
      "Age          891 non-null int64\n",
      "Fare         891 non-null int64\n",
      "Embarked     891 non-null int64\n",
      "Title        891 non-null float64\n",
      "IsAlone      891 non-null int64\n",
      "Age*Class    891 non-null int64\n",
      "dtypes: float64(1), int64(7)\n",
      "memory usage: 55.8 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns; sns.set()\n",
    "# sns.pairplot(train_df, hue='Survived', size=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classification\n",
    "Probabilistically determine the label for a new point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.8878923767\n",
      "72.5175632732\n",
      "74.8878923767\n",
      "72.5175632732\n",
      "74.8878923767\n",
      "72.5175632732\n",
      "peak memory: 130.38 MiB, increment: 0.25 MiB\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred= model.predict(X_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred)*100)\n",
    "print(cross_val_score(model, X, y, cv=10, scoring='accuracy').mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.4752298264\n",
      "80.4752298264\n",
      "peak memory: 130.81 MiB, increment: 0.42 MiB\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "model= LogisticRegression()\n",
    "\n",
    "#Fit Model\n",
    "\n",
    "scores= cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "print(scores.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esemble Method\n",
    "## Bagging\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "HyperParameters:\n",
    "- max_features: limits number of number of features in a tree\n",
    "- n_estimators: # of trees built before average prediciton is made\n",
    "- min_sample_leaf: End node of trees. Too small = more noise. For Regression tree.\n",
    "- n_jobs: computer processors utilized. -1 = no restrictions\n",
    "- random_state: seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0445749631\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=300, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "\n",
    "print(cross_val_score(bag, X, y, cv=10, scoring='accuracy').mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 280 candidates, totalling 1400 fits\n",
      "418\n",
      "80.6703910615\n",
      "{'max_features': 0.8, 'max_samples': 0.6, 'n_estimators': 40}\n",
      "peak memory: 135.72 MiB, increment: 3.34 MiB\n",
      "Wall time: 1min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "param_grid ={'max_features': [.2,.3,.4, .5,.6,.8, 1],\n",
    "             'n_estimators': [40,50,60,80,100,120,150,180],\n",
    "             'max_samples': [0.2,0.3,.4,.5,.6]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "#bag = BaggingClassifier(tree)\n",
    "\n",
    "# Stratified Cross Validation    \n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(BaggingClassifier(tree, n_jobs=1),\n",
    "                    param_grid, cv=cv, scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"Bagger_ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85858585858585856"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Titanic/Pickle/Bagger_ensemble.pickle', 'rb') as f: Bagging_ensemble = pickle.load(f)\n",
    "Bagging_ensemble.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829596412556\n",
      "peak memory: 142.95 MiB, increment: 7.23 MiB\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "model = RandomForestClassifier(n_estimators=500)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['criterion', 'oob_score', 'random_state', 'n_jobs', 'max_features', 'min_samples_leaf', 'max_leaf_nodes', 'max_depth', 'n_estimators', 'min_samples_split', 'class_weight', 'warm_start', 'bootstrap', 'min_impurity_split', 'min_weight_fraction_leaf', 'verbose'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   23.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "80.1117318436\n",
      "{'max_depth': 7, 'n_estimators': 400, 'max_features': 0.6, 'max_leaf_nodes': 10}\n",
      "peak memory: 142.96 MiB, increment: 0.00 MiB\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "param_grid ={'n_estimators':[400,500],\n",
    "             'max_features':[.7,.6],\n",
    "            'max_depth': [6,7,8],\n",
    "            'max_leaf_nodes':[10]}\n",
    "#param_grid ={'n_estimators':[200]}\n",
    "\n",
    "from sklearn import feature_selection\n",
    "\n",
    "# Stratified Cross Validation    \n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "#model = feature_selection.RFE(RandomForestClassifier())\n",
    "model= RandomForestClassifier()\n",
    "\n",
    "grid = GridSearchCV(model,\n",
    "                    param_grid, cv=cv,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"Random_Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier\n",
    "\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 96 candidates, totalling 672 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 672 out of 672 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "81.9632881085\n",
      "{'loss': 'exponential', 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}\n",
      "peak memory: 138.50 MiB, increment: 0.31 MiB\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "param_grid ={'n_estimators':[150,200,250,275],\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'learning_rate':[0.3,0.2,0.1,0.5],\n",
    "            'max_depth': [2,3,4]}\n",
    "\n",
    "# Stratified Cross Validation    \n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=7, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(GradientBoostingClassifier(),\n",
    "                    param_grid,cv=cv,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"Gradient_Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   16.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "79.4413407821\n",
      "{'learning_rate': 0.5, 'n_estimators': 250}\n",
      "peak memory: 138.50 MiB, increment: 0.00 MiB\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "param_grid ={'n_estimators':[150,200,250,275],\n",
    "            'learning_rate':[0.3,0.5,0.65,0.8]}\n",
    "\n",
    "# Stratified Cross Validation    \n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(AdaBoostClassifier(),\n",
    "                    param_grid,cv=cv, scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y);\n",
    "save(grid.best_estimator_, \"AdaBoost_Ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Add Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 38 candidates, totalling 190 fits\n",
      "418\n",
      "78.2122905028\n",
      "{'n_neighbors': 8, 'weights': 'uniform'}\n",
      "peak memory: 138.75 MiB, increment: 0.25 MiB\n",
      "Wall time: 1.92 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 190 out of 190 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "param_grid ={'n_neighbors': list(range(1,20)),\n",
    "            'weights':['uniform','distance']\n",
    "            }\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(KNeighborsClassifier(),\n",
    "                    param_grid,cv=cv, scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Classification\n",
    "Model new points by seeing where it falls upon a divide.\n",
    "Fast prediction phase, work well in high dimensional data, versatile\n",
    "\n",
    "Costly at high quantities of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "Creates a linear divide between point to classify. Maximizes the distance of the discriminatory margin.\n",
    "\n",
    "Hyperparameters:\n",
    "- C: Hardness of the margin. Higher C, less softening.\n",
    "\n",
    "\n",
    "Radial Basis Function (RBF)\n",
    "- Gamma: how far the influence of a single training example raches. low=far, high=close, Inverse of the radius of influence of samples selected by the model as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.3590965838\n",
      "peak memory: 138.75 MiB, increment: 0.01 MiB\n",
      "Wall time: 928 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# Define Model\n",
    "model = svm.LinearSVC()\n",
    "#Fit Model\n",
    "scores= cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "print(scores.mean()*100)\n",
    "\n",
    "#submit(svm.LinearSVC(), name=\"80linear_svc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "77.3184357542\n",
      "{'kernel': 'linear', 'C': 50}\n",
      "peak memory: 138.85 MiB, increment: 0.09 MiB\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "param_grid = [\n",
    "  {'C': [50,100], 'kernel': ['linear']},\n",
    "  {'C': [150], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}\n",
    " ]\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(svm.SVC(),\n",
    "                    param_grid, cv=cv,\n",
    "                    scoring='accuracy', verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"SVCdouble_kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-02   1.77827941e-01   3.16227766e+00   5.62341325e+01\n",
      "   1.00000000e+03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e-09,   1.00000000e-08,   1.00000000e-07,\n",
       "         1.00000000e-06,   1.00000000e-05,   1.00000000e-04,\n",
       "         1.00000000e-03,   1.00000000e-02,   1.00000000e-01,\n",
       "         1.00000000e+00,   1.00000000e+01,   1.00000000e+02,\n",
       "         1.00000000e+03])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.logspace(-2, 3, 5))\n",
    "np.logspace(-9, 3, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "78.1005586592\n",
      "{'C': 100000.0, 'gamma': 0.0001}\n",
      "peak memory: 138.85 MiB, increment: 0.00 MiB\n",
      "Wall time: 3.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "C_range = np.logspace(2, 5, 7)\n",
    "gamma_range = np.logspace(-4, 5, 1)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(svm.SVC(),\n",
    "                    param_grid, cv=cv,\n",
    "                    scoring='accuracy', verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"SVC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + SVC Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16 s\n",
      "418\n",
      "78.7709497207\n",
      "{'svc__C': 50, 'svc__gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(n_components=6, whiten=True, random_state=42, svd_solver='randomized')\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)\n",
    "# pipeline!\n",
    "\n",
    "param_grid = [{'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]},\n",
    "              {'svc__C': [0.001, 0.01, 0.1, 1, 10, 50, 100, 150, 1000, 1500], 'svc__kernel': ['linear']}]\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(model, param_grid, cv=cv)\n",
    "\n",
    "%time grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"PCA_SVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"Titanic/results.csv\", 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"Titanic/results.csv\").transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(80.67039106145252, 'Bagger_ensemble', {'max_features': 0.8, 'max_samples': 0.6, 'n_estimators': 40})</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(80.111731843575413, 'Random_Forest', {'max_depth': 7, 'n_estimators': 400, 'max_features': 0.6, 'max_leaf_nodes': 10})</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(81.963288108539501, 'Gradient_Boosting', {'loss': 'exponential', 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250})</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(79.441340782122907, 'AdaBoost_Ensemble', {'learning_rate': 0.5, 'n_estimators': 250})</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(78.212290502793294, 'KNN', {'n_neighbors': 8, 'weights': 'uniform'})</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(77.318435754189935, 'SCVdouble_kernel', {'kernel': 'linear', 'C': 50})</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(78.100558659217882, 'SVC2', {'C': 100000.0, 'gamma': 0.0001})</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(78.770949720670387, 'PCA_SVC', {'svc__C': 50, 'svc__gamma': 0.001})</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [(80.67039106145252, 'Bagger_ensemble', {'max_features': 0.8, 'max_samples': 0.6, 'n_estimators': 40}), (80.111731843575413, 'Random_Forest', {'max_depth': 7, 'n_estimators': 400, 'max_features': 0.6, 'max_leaf_nodes': 10}), (81.963288108539501, 'Gradient_Boosting', {'loss': 'exponential', 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}), (79.441340782122907, 'AdaBoost_Ensemble', {'learning_rate': 0.5, 'n_estimators': 250}), (78.212290502793294, 'KNN', {'n_neighbors': 8, 'weights': 'uniform'}), (77.318435754189935, 'SCVdouble_kernel', {'kernel': 'linear', 'C': 50}), (78.100558659217882, 'SVC2', {'C': 100000.0, 'gamma': 0.0001}), (78.770949720670387, 'PCA_SVC', {'svc__C': 50, 'svc__gamma': 0.001})]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MODEL ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_file = open(\"Titanic/Pickle/AdaBoost_Ensemble.pickle\", \"rb\")\n",
    "Ada_Boost_Ensemble = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(\"Titanic/Pickle/Bagger_ensemble.pickle\", \"rb\")\n",
    "Bagger_ensemble = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(\"Titanic/Pickle/Gradient_Boosting.pickle\", \"rb\")\n",
    "Gradient_Boosting = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(\"Titanic/Pickle/KNN.pickle\", \"rb\")\n",
    "KNN = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(\"Titanic/Pickle/PCA_SVC.pickle\", \"rb\")\n",
    "PCA_SVC = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(\"Titanic/Pickle/Random_Forest.pickle\", \"rb\")\n",
    "Random_Forest = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(\"Titanic/Pickle/SVCdouble_kernel.pickle\", \"rb\")\n",
    "SVCdouble_kernel = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(\"Titanic/Pickle/SVC2.pickle\", \"rb\")\n",
    "SVC2 = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "?VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('ada', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.5, n_estimators=250, random_state=None)), ('bagger', BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, ...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('ada',Ada_Boost_Ensemble),\n",
    "    ('bagger',Bagger_ensemble),\n",
    "    ('gradientboost',Gradient_Boosting),\n",
    "    #('knn',KNN),\n",
    "    #('pca_svc',PCA_SVC)\n",
    "    ('svc1',SVCdouble_kernel),\n",
    "    ('svc2',SVC2) \n",
    "], voting='soft')\n",
    "                           # weights=[2,2,2,1,1,1,1])\n",
    "                            #flatten_transform=True)\n",
    "ensemble.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-cb04ec3e8be9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(ensemble.transform(X).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                  \" voting=%r\" % self.voting)\n\u001b[0;32m    209\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36m_collect_probas\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[0;32m    583\u001b[0m                                  \" probability=False\")\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'c_svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nu_svc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "print(ensemble.predict_proba(X))\n",
    "#print(ensemble.transform(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a493c4f4754c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'hard' voting\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m             maj = np.apply_along_axis(lambda x:\n\u001b[0;32m    191\u001b[0m                                       np.argmax(np.bincount(x,\n",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\voting_classifier.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[1;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                 \u001b[1;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m             )\n\u001b[0;32m    345\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 6, n_neighbors = 8"
     ]
    }
   ],
   "source": [
    "ensemble.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensembling(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.PassengerId, \n",
    "                           'Survived':submission})\n",
    "    print(len(df))\n",
    "    df.to_csv((\"Titanic/submissions/{}.csv\".format(modelname)),header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n"
     ]
    }
   ],
   "source": [
    "ensembling(ensemble, \"ensemble1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 2 2 2]\n",
      "[1 1 1 2 2 2]\n",
      "[1 1 1 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "    ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "eclf1 = eclf1.fit(X, y)\n",
    "print(eclf1.predict(X))\n",
    "eclf2 = VotingClassifier(estimators=[\n",
    "    ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "                         voting='soft')\n",
    "eclf2 = eclf2.fit(X, y)\n",
    "print(eclf2.predict(X))\n",
    "eclf3 = VotingClassifier(estimators=[\n",
    "    ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "                         voting='soft', weights=[2,1,1])\n",
    "eclf3 = eclf3.fit(X, y)\n",
    "print(eclf3.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
