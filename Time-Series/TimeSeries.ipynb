{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series with Wikipedia\n",
    "https://www.kaggle.com/screech/ensemble-of-arima-and-lstm-model-for-wiki-pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicapotato/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re # to separate pages based on language (regular expression)\n",
    "import matplotlib.pyplot as plt # to visualize data\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA # to make an ARIMA model that fits the data\n",
    "\n",
    "from pandas.core import datetools\n",
    "from pandas.tools.plotting import autocorrelation_plot # to visualize and configure the parameters of ARIMA model\n",
    "\n",
    "# Working module\n",
    "from pandas import tseries\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/My Computer/DATA/Web_Traffic/train_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-66c96af4d064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"D:/My Computer/DATA/Web_Traffic\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_1.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/My Computer/DATA/Web_Traffic/train_1.csv'"
     ]
    }
   ],
   "source": [
    "path = r\"D:/My Computer/DATA/Web_Traffic\"\n",
    "train_df = pd.read_csv(open(os.path.join(path, \"train_1.csv\"), \"r\",encoding=\"utf-8\")).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df [:10000]\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_language(page):\n",
    "    res = re.search('[a-z][a-z].wikipedia.org',page)\n",
    "    if res:\n",
    "        return res.group(0)[0:2]\n",
    "    return 'na'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['lang'] = train_df.Page.map(lambda x: get_language(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(Counter(train_df.lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang_sets = {}\n",
    "lang_sets['en'] = train_df[train_df.lang=='en'].iloc[:,0:-1]\n",
    "lang_sets['ja'] = train_df[train_df.lang=='ja'].iloc[:,0:-1]\n",
    "lang_sets['de'] = train_df[train_df.lang=='de'].iloc[:,0:-1]\n",
    "lang_sets['na'] = train_df[train_df.lang=='na'].iloc[:,0:-1]\n",
    "lang_sets['fr'] = train_df[train_df.lang=='fr'].iloc[:,0:-1]\n",
    "lang_sets['zh'] = train_df[train_df.lang=='zh'].iloc[:,0:-1]\n",
    "lang_sets['ru'] = train_df[train_df.lang=='ru'].iloc[:,0:-1]\n",
    "lang_sets['es'] = train_df[train_df.lang=='es'].iloc[:,0:-1]\n",
    "\n",
    "sums = {}\n",
    "for key in lang_sets:\n",
    "    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = [r for r in range(sums['en'].shape[0])]\n",
    "\n",
    "fig = plt.figure(1,figsize=[10,10])\n",
    "plt.ylabel('Views per Page')\n",
    "plt.xlabel('Day')\n",
    "plt.title('Pages in Different Languages')\n",
    "labels={'en':'English','ja':'Japanese','de':'German',\n",
    "        'na':'Media','fr':'French','zh':'Chinese',\n",
    "        'ru':'Russian','es':'Spanish'\n",
    "       }\n",
    "\n",
    "for key in sums:\n",
    "    plt.plot(days,sums[key],label = labels[key] )\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive Integrated Moving Average (ARIMA)\n",
    "Regression of the variable against itsefl. \n",
    "\n",
    "ARIMA (p, d, q) model\n",
    "These are the key parameters of ARIMA and picking the right value for p, d, q will yield better model results.\n",
    "- p = order of the autoregressive part. That is the number of unknown terms that multiply your signal at past times (so many past times as your value p).\n",
    "- d = degree of first differencing involved. Number of times you have to difference your time series to have a stationary one.\n",
    "- q = order of the moving average part. That is the number of unknown terms that multiply your forecast errors at past times (so many past times as your value q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "for key in sums:\n",
    "    fig = plt.figure(1,figsize=[10,5])\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    data = np.array(sums[key])\n",
    "    autocorr = acf(data)\n",
    "    pac = pacf(data)\n",
    "\n",
    "    x = [x for x in range(len(pac))]\n",
    "    ax1.plot(x[1:],autocorr[1:])\n",
    "\n",
    "    ax2.plot(x[1:],pac[1:])\n",
    "    ax1.set_xlabel('Lag')\n",
    "    ax1.set_ylabel('Autocorrelation')\n",
    "\n",
    "    ax2.set_xlabel('Lag')\n",
    "    ax2.set_ylabel('Partial Autocorrelation')\n",
    "    print(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params = {'en': [4,1,0], 'ja': [7,1,1], 'de': [7,1,1], 'na': [4,1,0], 'fr': [4,1,0], 'zh': [7,1,1], 'ru': [4,1,0], 'es': [7,1,1]}\n",
    "\n",
    "for key in sums:\n",
    "    data = np.array(sums[key])\n",
    "    result = None\n",
    "    arima = ARIMA(data,params[key])\n",
    "    result = arima.fit(disp=False)\n",
    "    print(result.params)\n",
    "    pred = result.predict(2,599,typ='levels')\n",
    "    x = [i for i in range(600)]\n",
    "    i=0\n",
    "    \n",
    "    print(key)\n",
    "    plt.plot(x[2:len(data)],data[2:] ,label='Data')\n",
    "    plt.plot(x[2:],pred,label='ARIMA Model')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Views')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages for pre processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    " # Importing the Keras libraries and packages for LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in sums:\n",
    "    row = [0]*sums[key].shape[0]\n",
    "    for i in range(sums[key].shape[0]):\n",
    "        row[i] = sums[key][i]\n",
    "\n",
    "\n",
    "    #Using Data From Random Row for Training and Testing\n",
    "\n",
    "    X = row[0:549]\n",
    "    y = row[1:550]\n",
    "\n",
    "    # Splitting the dataset into the Training set and Test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "    # Feature Scaling\n",
    "    sc = MinMaxScaler()\n",
    "    X_train = np.reshape(X_train,(-1,1))\n",
    "    y_train = np.reshape(y_train,(-1,1))\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    y_train = sc.fit_transform(y_train)\n",
    "\n",
    "\n",
    "    #Training LSTM\n",
    "\n",
    "    #Reshaping Array\n",
    "    X_train = np.reshape(X_train, (384,1,1))\n",
    "\n",
    "    # Initialising the RNN\n",
    "    regressor = Sequential()\n",
    "\n",
    "    # Adding the input layerand the LSTM layer\n",
    "    regressor.add(LSTM(units = 8, activation = 'relu', input_shape = (None, 1)))\n",
    "\n",
    "\n",
    "    # Adding the output layer\n",
    "    regressor.add(Dense(units = 1))\n",
    "\n",
    "    # Compiling the RNN\n",
    "    regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error')\n",
    "\n",
    "    # Fitting the RNN to the Training set\n",
    "    regressor.fit(X_train, y_train, batch_size = 10, epochs = 100, verbose = 0)\n",
    "\n",
    "    # Getting the predicted Web View\n",
    "    inputs = X\n",
    "    inputs = np.reshape(inputs,(-1,1))\n",
    "    inputs = sc.transform(inputs)\n",
    "    inputs = np.reshape(inputs, (549,1,1))\n",
    "    y_pred = regressor.predict(inputs)\n",
    "    y_pred = sc.inverse_transform(y_pred)\n",
    "\n",
    "    print(key)\n",
    "    #Visualising Result\n",
    "    plt.figure\n",
    "    plt.plot(y, color = 'red', label = 'Real Web View')\n",
    "    plt.plot(y_pred, color = 'blue', label = 'Predicted Web View')\n",
    "    plt.title('Web View Forecasting')\n",
    "    plt.xlabel('Number of Days from Start')\n",
    "    plt.ylabel('Web View')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
