import numpy as np
import pandas as pd
import nltk
import random
import csv

# Read
data= pd.read_csv("modeldata.csv", encoding='latin-1')
print(data[:1])

def find_features(document):
    words = set(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)

    return features
data.head()

labtext= list(zip(data.text, (data.LABEL))) 
# Tuple each review with its label (1= greater/equal to 3, 0= under 3)
# Apply function to data
featuresets = [(find_features(text), LABEL) for (text, LABEL) in labtext]
len(featuresets)


# In[114]:

training_set = featuresets[:15000]
testing_set = featuresets[15000:]


# ## Naive Bayes

# In[115]:

classifier = nltk.NaiveBayesClassifier.train(training_set) 
# Very scalable algorithm\n# Posterior = prior_occurence * likelihood / evidence'


# In[ ]:

classifier_f = open("naivebayes.pickle", "rb")
classifier = pickle.load(classifier_f)
classifier_f.close()


# In[116]:

print("Classifier accuracy percent:",(nltk.classify.accuracy(classifier, testing_set))*100)


# In[117]:

classifier.show_most_informative_features(25)


# In[118]:

import pickle


# In[119]:

save_classifier = open("naivebayes.pickle","wb")
pickle.dump(classifier, save_classifier)
save_classifier.close()
