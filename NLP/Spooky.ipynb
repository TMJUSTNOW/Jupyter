{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spooky Spooky Exploration\n",
    "By Nick Brooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "# Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from subprocess import check_output\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns\n",
    "\n",
    "# Pre-Processing\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "os.chdir(r\"C:\\Users\\Nicol\\Google Drive\\Learning\\Jupyter\\Data\\spooky\")\n",
    "df = pd.read_csv(\"train.csv\", index_col=\"id\")\n",
    "test = pd.read_csv(\"test.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 500)\n",
    "df.text= df.text.astype(str)\n",
    "df.author = pd.Categorical(df.author)\n",
    "df.iloc[:20,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "#ps = LancasterStemmer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocessing(data):\n",
    "    txt = data.str.lower().str.cat(sep=' ') #1\n",
    "    words = tokenizer.tokenize(txt) #2\n",
    "    words = [w for w in words if not w in stop_words] #3\n",
    "    #ords = [ps.stem(w) for w in words] #4\n",
    "    return words\n",
    "\n",
    "def wordfreqviz(text, x):\n",
    "    word_dist = nltk.FreqDist(text)\n",
    "    top_N = x\n",
    "    rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "    matplotlib.style.use('ggplot')\n",
    "    rslt.plot.bar(rot=0)\n",
    "\n",
    "def wordfreq(text, x):\n",
    "    word_dist = nltk.FreqDist(text)\n",
    "    top_N = x\n",
    "    rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency']).set_index('Word')\n",
    "    print(rslt)\n",
    "\n",
    "# Sources\n",
    "# https://www.kaggle.com/longdoan/word-cloud-with-python\n",
    "# https://github.com/amueller/word_cloud/issues/134\n",
    "# https://amueller.github.io/word_cloud/auto_examples/masked.html\n",
    "\n",
    "def cloud(text, title):\n",
    "    # Setting figure parameters\n",
    "    mpl.rcParams['figure.figsize']=(10.0,10.0)    #(6.0,4.0)\n",
    "    mpl.rcParams['font.size']=12                #10 \n",
    "    mpl.rcParams['savefig.dpi']=100             #72 \n",
    "    mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "    \n",
    "    # Processing Text\n",
    "    stopwords = set(STOPWORDS) # Redundant\n",
    "    wordcloud = WordCloud(width=1600, height=800,\n",
    "                          background_color='black',\n",
    "                          stopwords=stopwords,\n",
    "                         ).generate(str(text))\n",
    "\n",
    "    print(wordcloud);\n",
    "    \n",
    "    # Output Visualization\n",
    "    fig = plt.figure(figsize=(20,10), facecolor='k')\n",
    "    plt.title(title)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "    plt.show();\n",
    "    #fig.savefig(\"wordcloud.png\", dpi=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Could study the word mood excuded by the various authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pre-Processing\n",
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ('EAP', 'HPL', 'MWS'):\n",
    "    print(x)\n",
    "    print(cloud(preprocessing(df[df.author == x]['text']),x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would be interesting to extract the various types of Parts of Speech, in order to hone in on the different choie in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams((text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "def gramfreq(text,n,num):\n",
    "    # Extracting bigrams\n",
    "    result = get_ngrams(text,n)\n",
    "    # Counting bigrams\n",
    "    result_count = Counter(result)\n",
    "    # Converting to the result to a data frame\n",
    "    df = pd.DataFrame.from_dict(result_count, orient='index')\n",
    "    df = df.rename(columns={'index':'words', 0:'frequency'}) # Renaming index column name\n",
    "    #----output----\n",
    "    print(df.sort_values([\"frequency\"],ascending=[0])[:num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams((text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "def gramfreq(text,n,num):\n",
    "    # Extracting bigrams\n",
    "    result = get_ngrams(text,n)\n",
    "    # Counting bigrams\n",
    "    result_count = Counter(result)\n",
    "    # Converting to the result to a data frame\n",
    "    df = pd.DataFrame.from_dict(result_count, orient='index')\n",
    "    df = df.rename(columns={'index':'words', 0:'frequency'}) # Renaming index column name\n",
    "    \n",
    "    return df.sort_values([\"frequency\"],ascending=[0])[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_table(x, gram, length):\n",
    "    out = pd.DataFrame(index=None)\n",
    "    for i in gram:\n",
    "        table = pd.DataFrame(gramfreq(preprocessing(df[df.author == x]['text']),i,length).reset_index())\n",
    "        table.columns = [\"{}-Gram\".format(i),\"Occurence\"]\n",
    "        out = pd.concat([out, table], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try to use pd.crosstab\n",
    "# https://www.kaggle.com/ash316/eda-to-prediction-dietanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_table(x=\"EAP\", gram=[1,2,3,4], length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_table(x=\"HPL\", gram=[1,2,3,4], length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_table(x=\"MWS\", gram=[1,2,3,4], length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Generative Models based off Bayes' Rule and Conditional Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Vocabulary Size: {}\".format(len(nltk.FreqDist(preprocessing(df['text'])))))\n",
    "print(\"Train Size: {}\".format(len(df)))\n",
    "print(\"Test Vocabulary Size: {}\".format(len(nltk.FreqDist(preprocessing(test['text'])))))\n",
    "print(\"Test Size: {}\".format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features\n",
    "all_words = nltk.FreqDist(preprocessing(df['text'])) # Calculate word occurence from whole block of text\n",
    "word_features = list(all_words.keys())[:20000] \n",
    "# Number of columns (can't exceed vocab, only shrink it) from largest to smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "# for each review, records which uniqeue words out of the whole text body are present\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to create model features\n",
    "def model_prep(state, df_in):\n",
    "    df_in['tokenized'] = df_in.text.astype(str).str.lower() # turn into lower case text\n",
    "    df_in['tokenized'] = df_in.apply(lambda row: tokenizer.tokenize(row['tokenized']), axis=1) # apply tokenize to each row\n",
    "    df_in['tokenized'] = df_in['tokenized'].apply(lambda x: [w for w in x if not w in stop_words]) # remove stopwords from each row\n",
    "    df_in['tokenized'] = df_in['tokenized'].apply(lambda x: [ps.stem(w) for w in x]) # apply stemming to each row\n",
    "    if state == \"Train\":\n",
    "        print(\"{} Word Features: {}\".format(state, len(word_features)))\n",
    "        print(\"All Possible words in {} set: {}\".format(state, len(all_words)))\n",
    "        # Bag of Words with Label\n",
    "        featuresets = [(find_features(text), LABEL) for (text, LABEL) in list(zip(df_in.tokenized, (df_in.author)))]\n",
    "        print(\"Train Set Size: {}\".format(len(featuresets)))\n",
    "        print(\"Train Set Ready\")\n",
    "        return featuresets, word_features\n",
    "    else:\n",
    "        # Bag of Words without Labels\n",
    "        featuresets = [(find_features(text)) for (text) in list(df_in.tokenized)]\n",
    "        print(\"Submission Set Size: {}\".format(len(featuresets)))\n",
    "        print(\"Submission Set Ready\")\n",
    "        return featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, word_features= model_prep(\"Train\", df_in=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionset = model_prep(\"Test\", df_in=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = trainset[:15000]\n",
    "testing_set = trainset[15000:]\n",
    "del trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start = time.time()\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "# Posterior = prior_occurence * likelihood / evidence\n",
    "end = time.time()\n",
    "print(\"Model took %0.2f seconds to train\"%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Edgar Allan Poe [EAP], Mary Shelley[MWS], and HP Lovecraft[HPL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Classifier Test Accuracy:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "print(classifier.show_most_informative_features(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = classifier.labels()\n",
    "submission = pd.DataFrame(columns=labels)\n",
    "for x in submissionset:\n",
    "    dist = classifier.prob_classify(x)\n",
    "    submission= submission.append({labels[0]:dist.prob(labels[0]),\n",
    "                                   labels[1]:dist.prob(labels[1]),\n",
    "                                   labels[2]:dist.prob(labels[2])},ignore_index=True)\n",
    "submission.index = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"naive_spooky.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
