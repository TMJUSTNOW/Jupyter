{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import multiprocessing\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "# machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#Evalaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Grid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Performance\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#os.chdir(r\"D:/My Computer/\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Number Recognition\n",
    "path = r\"C:/Users/Nicol/Google Drive/Learning/Jupyter/Number_Recognition\"\n",
    "train_df = pd.read_csv(open(os.path.join(path, \"train.csv\"), \"r\")) \n",
    "oosample = pd.read_csv(open(os.path.join(path, \"test.csv\"), \"r\")) \n",
    "\n",
    "train_df = train_df.take(np.random.permutation(len(train_df)))\n",
    "\n",
    "X = train_df.iloc[:,1:]\n",
    "y = train_df.iloc[:,0]\n",
    "print(X.shape, y.shape, oosample.shape)\n",
    "\n",
    "results=[]\n",
    "def save(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "\n",
    "    df = pd.DataFrame({'ImageId':list(range(1, len(submission)+1)), \n",
    "                       'Label':submission})\n",
    "\n",
    "    print(len(df))\n",
    "    \n",
    "    df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    \n",
    "    # CV and Save Scores\n",
    "    trainingscore = (grid.best_score_*100)\n",
    "    results.append([(trainingscore),(\"{}\".format(modelname)), grid.best_estimator_])\n",
    "    print(trainingscore)\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)\n",
    "    \n",
    "def ensembling(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "    df = pd.DataFrame({'ImageId':list(range(1, len(submission)+1)), \n",
    "                       'Label':submission})\n",
    "    print(len(df))\n",
    "    df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11) (891,) (418, 11)\n"
     ]
    }
   ],
   "source": [
    "#Titanic\n",
    "path = r\"C:/Users/Nicol/Google Drive/Learning/Jupyter/Titanic\"\n",
    "#train_df = pd.read_csv(open(os.path.join(path, \"clean_train.csv\"), \"r\")) \n",
    "#test_df = pd.read_csv(open(os.path.join(path, \"clean_test.csv\"), \"r\"))\n",
    "\n",
    "train_df = pd.read_csv(open(os.path.join(path, \"clean_train2.csv\"), \"r\")) \n",
    "test_df = pd.read_csv(open(os.path.join(path, \"clean_test2.csv\"), \"r\")) \n",
    "\n",
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "\n",
    "oosample  = test_df.drop([\"PassengerId\"] , axis=1).copy()\n",
    "print(X.shape, y.shape, oosample.shape)\n",
    "\n",
    "results=[]\n",
    "def save(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.PassengerId, \n",
    "                           'Survived':submission})\n",
    "    df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    \n",
    "    # CV and Save Scores\n",
    "    trainingscore = (grid.best_score_*100)\n",
    "    results.append([(trainingscore),(\"{}\".format(modelname)), grid.best_estimator_])\n",
    "    print(trainingscore)\n",
    "    print(grid.best_params_)\n",
    "\n",
    "    with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)\n",
    "        \n",
    "def norm_save(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.PassengerId, \n",
    "                           'Survived':submission})\n",
    "    df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)\n",
    "\n",
    "def ensembling(model, modelname):\n",
    "    model.fit(X, y)\n",
    "    submission = model.predict(oosample)\n",
    "    df = pd.DataFrame({'PassengerId':test_df.PassengerId, \n",
    "                           'Survived':submission})\n",
    "    print(len(df))\n",
    "    df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n",
    "    with open((os.path.join(path,(r\"Pickle/{}.pickle\".format(modelname)))), 'wb') as f: pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use train/test split with different random_state values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "\n",
    "# Stratified Cross Validation\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normal CV\n",
    "#cv =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      "Pclass         891 non-null int64\n",
      "Sex            891 non-null int64\n",
      "Age            891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Fare           891 non-null int64\n",
      "Embarked       891 non-null int64\n",
      "Name_length    891 non-null int64\n",
      "Has_Cabin      891 non-null int64\n",
      "FamilySize     891 non-null int64\n",
      "IsAlone        891 non-null int64\n",
      "Title          891 non-null int64\n",
      "dtypes: int64(11)\n",
      "memory usage: 76.6 KB\n",
      "None\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      "Pclass         418 non-null int64\n",
      "Sex            418 non-null int64\n",
      "Age            418 non-null int64\n",
      "Parch          418 non-null int64\n",
      "Fare           418 non-null int64\n",
      "Embarked       418 non-null int64\n",
      "Name_length    418 non-null int64\n",
      "Has_Cabin      418 non-null int64\n",
      "FamilySize     418 non-null int64\n",
      "IsAlone        418 non-null int64\n",
      "Title          418 non-null int64\n",
      "dtypes: int64(11)\n",
      "memory usage: 36.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(X.info())\n",
    "print(y.head())\n",
    "print(oosample.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns; sns.set()\n",
    "# sns.pairplot(train_df, hue='Survived', size=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Classification\n",
    "Probabilistically determine the label for a new point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.779046929974\n",
      "0.779046929974\n",
      "0.779046929974\n",
      "peak memory: 132.99 MiB, increment: 0.01 MiB\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "model = GaussianNB()\n",
    "\n",
    "score = cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "print(score.mean())\n",
    "norm_save(GaussianNB(), \"Gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797933832709\n",
      "0.797933832709\n",
      "peak memory: 133.41 MiB, increment: 0.42 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "model= LogisticRegression()\n",
    "score = cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "print(score.mean())\n",
    "norm_save(LogisticRegression(), \"Logistic_Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esemble Method\n",
    "\n",
    "Means that a bunch of the model get created and are aggregated at the end for best performance.\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Aka Bootstrap- creates a bunch of trees using a random 3/4 the the data for each, while using sampling without replacement, which means that values may be sampled multiple times.\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "HyperParameters:\n",
    "- max_features: This is the random subset of features to be used for splitting node, the lower the better to reduce variance. For Classification model, ideal max_features = sqr(n_var)\n",
    "- n_estimators: # of trees built before average prediciton is made\n",
    "- min_sample_leaf: End node of trees. Too small = more noise. For Regression tree.\n",
    "- n_jobs: computer processors utilized. -1 = no restrictions\n",
    "- random_state: seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.2705708773\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=300, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "\n",
    "print(cross_val_score(bag, X, y, cv=10, scoring='accuracy').mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.6567164179\n",
      "{'n_estimators': 445}\n",
      "peak memory: 144.98 MiB, increment: 11.08 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "param_grid ={'n_estimators': np.arange(20, 500, 25)}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "#bag = BaggingClassifier(tree)\n",
    "\n",
    "grid = GridSearchCV(BaggingClassifier(tree),\n",
    "                    param_grid, cv=cv, scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"Bagger_ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Trees are created wih a randomly picked subset of observations and variables. More uncorrelated splits, less overemphasis on certain features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810055865922\n",
      "peak memory: 156.43 MiB, increment: 11.45 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "model = RandomForestClassifier(n_estimators=500)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['class_weight', 'max_leaf_nodes', 'random_state', 'min_impurity_split', 'max_features', 'n_jobs', 'verbose', 'warm_start', 'min_samples_leaf', 'n_estimators', 'min_weight_fraction_leaf', 'min_samples_split', 'max_depth', 'bootstrap', 'oob_score', 'criterion'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 560 candidates, totalling 2800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 2800 out of 2800 | elapsed: 16.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.1940298507\n",
      "{'max_leaf_nodes': 8, 'max_depth': 7, 'n_estimators': 425, 'max_features': 0.55000000000000004}\n",
      "peak memory: 156.43 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "param_grid ={'max_depth': np.arange(6, 11, 1),\n",
    "             'n_estimators':np.arange(350, 450, 25),\n",
    "             'max_features':np.arange(0.5,.81, 0.05),\n",
    "            'max_leaf_nodes':np.arange(6, 10, 1)}\n",
    "#param_grid ={'n_estimators':[200]}\n",
    "\n",
    "from sklearn import feature_selection\n",
    "\n",
    "#model = feature_selection.RFE(RandomForestClassifier())\n",
    "model= RandomForestClassifier()\n",
    "\n",
    "grid = GridSearchCV(model,\n",
    "                    param_grid, cv=cv,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"Random_Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees (ExtraTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier\n",
    "\n",
    "Method, similarly to deep learning, applies weights to all data points and optimizes them using the loss function. Fixes mistakes by assigning high weights to them during iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 88 candidates, totalling 440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 440 out of 440 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.5223880597\n",
      "{'learning_rate': 0.10000000000000001, 'n_estimators': 150}\n",
      "peak memory: 148.86 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "param_grid ={'n_estimators':np.arange(50, 301, 25),\n",
    "            'learning_rate':np.arange(.1, 4, .5)}\n",
    "\n",
    "grid = GridSearchCV(AdaBoostClassifier(),\n",
    "                    param_grid,cv=cv, scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y);\n",
    "save(grid.best_estimator_, \"AdaBoost_Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier\n",
    "\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions.\n",
    "\n",
    "Part of the generalized boosting algorithms. Can use more loss functions than AdaBoost, and uses gradients instead of high-weight data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 630 candidates, totalling 3150 fits\n",
      "81.3432835821\n",
      "{'learning_rate': 0.060000000000000005, 'max_depth': 2.0, 'loss': 'deviance', 'n_estimators': 100}\n",
      "peak memory: 148.86 MiB, increment: 2.79 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 3150 out of 3150 | elapsed:  4.6min finished\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "param_grid ={'n_estimators':np.arange(100, 301, 25),\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'learning_rate':np.arange(0.01, 0.32,.05),\n",
    "            'max_depth': np.arange(2, 4.1, .5)}\n",
    "\n",
    "grid = GridSearchCV(GradientBoostingClassifier(),\n",
    "                    param_grid,cv=cv,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"Gradient_Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#?KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "77.3134328358\n",
      "{'n_neighbors': 5, 'weights': 'distance'}\n",
      "peak memory: 149.38 MiB, increment: 0.52 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "param_grid ={'n_neighbors': np.arange(1,21,1),\n",
    "            'weights':['uniform','distance']\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(),\n",
    "                    param_grid,cv=cv, scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Classification\n",
    "Model new points by seeing where it falls upon a divide.\n",
    "Fast prediction phase, work well in high dimensional data, versatile\n",
    "\n",
    "Costly at high quantities of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "65.223880597\n",
      "{'loss': 'hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "71.2686567164\n",
      "{'loss': 'modified_huber'}\n",
      "peak memory: 149.43 MiB, increment: 0.04 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "param_grid ={'loss':[\"hinge\",\"log\",\"modified_huber\",\"squared_hinge\",\"epsilon_insensitive\",\"squared_epsilon_insensitive\"]\n",
    "            }\n",
    "\n",
    "grid = GridSearchCV(SGDClassifier(),\n",
    "                    param_grid,cv=cv, scoring='accuracy',\n",
    "                    verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"StochasticGradientDescent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "Creates a linear divide between point to classify. Maximizes the distance of the discriminatory margin.\n",
    "\n",
    "Hyperparameters:\n",
    "- C: Hardness of the margin. Higher C, less softening.\n",
    "\n",
    "\n",
    "Radial Basis Function (RBF)\n",
    "- Gamma: how far the influence of a single training example raches. low=far, high=close, Inverse of the radius of influence of samples selected by the model as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.3327658609\n",
      "peak memory: 149.43 MiB, increment: 0.01 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "\n",
    "# Define Model\n",
    "model = svm.LinearSVC()\n",
    "#Fit Model\n",
    "scores= cross_val_score(model, X, y, cv=10, scoring='accuracy')\n",
    "print(scores.mean()*100)\n",
    "\n",
    "#submit(svm.LinearSVC(), name=\"80linear_svc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 310 candidates, totalling 1550 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1550 out of 1550 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.4469273743\n",
      "{'gamma': 0.016681005372000592, 'kernel': 'rbf', 'probability': True, 'C': 65}\n",
      "peak memory: 152.59 MiB, increment: 3.16 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "param_grid = [\n",
    "  {'C': np.arange(25,176,5), 'gamma': np.logspace(1, -4, 10), 'kernel': ['rbf'],\"probability\" : [True]}\n",
    " ]\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(svm.SVC(),\n",
    "                    param_grid, cv=cv,\n",
    "                    scoring='accuracy', verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"SVCrbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.8805970149\n",
      "{'kernel': 'linear', 'C': 1, 'probability': True}\n",
      "peak memory: 135.85 MiB, increment: 2.44 MiB\n"
     ]
    }
   ],
   "source": [
    "%%memit\n",
    "param_grid = {'C': [1,10],'kernel':['linear'], \"probability\" : [True]}\n",
    "\n",
    "grid = GridSearchCV(svm.SVC(),\n",
    "                    param_grid, cv=cv,\n",
    "                    scoring='accuracy', verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"SVCLinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + SVC Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n",
      "80.8938547486\n",
      "{'svc__C': 1.0, 'svc__gamma': 0.1668100537200059}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 350 out of 350 | elapsed:   19.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(n_components=5, whiten=True, random_state=42, svd_solver='randomized')\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)\n",
    "# pipeline!\n",
    "\n",
    "param_grid = [{'svc__C': np.logspace(-2, 3, 6),\n",
    "              'svc__gamma': np.logspace(1, -7, 10)},\n",
    "              {'svc__C': [0.001, 0.01, 0.1, 1, 10, 50, 100, 150, 1000, 1500], 'svc__kernel': ['linear']}]\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(model, param_grid, cv=cv, verbose=1)\n",
    "\n",
    "grid.fit(X, y)\n",
    "save(grid.best_estimator_, \"PCA_SVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(os.path.join(path, \"results.csv\"), 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(open(os.path.join(path, \"results.csv\"), \"r\")).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[80.89385474860336, 'PCA_SVC', Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=42,\\n  svd_solver='randomized', tol=0.0, whiten=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\\n  decision_function_shape=None, degree=3, gamma=0.1668100537200059,\\n  kernel='rbf', max_iter=-1, probability=False, random_state=None,\\n  shrinking=True, tol=0.001, verbose=False))])]</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [[80.89385474860336, 'PCA_SVC', Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=42,\n",
       "  svd_solver='randomized', tol=0.0, whiten=True)), ('svc', SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.1668100537200059,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MODEL ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WindowsApps\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# set seed for reproducability\n",
    "np.random.seed(2017)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# currently its available as part of mlxtend and not sklearn\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.10000000000000001, n_estimators=150,\n",
      "          random_state=None)\n"
     ]
    }
   ],
   "source": [
    "open_file = open(os.path.join(path,\"Pickle/AdaBoost_Ensemble.pickle\"), \"rb\")\n",
    "Ada_Boost_Ensemble = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "print(Ada_Boost_Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_file = open(os.path.join(path,\"Pickle/Logistic_Regression.pickle\"), \"rb\")\n",
    "Logistic_Regression = pickle.load(open_file)\n",
    "open_file.close()    \n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/Random_Forest.pickle\"), \"rb\")\n",
    "Random_Forest = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/SVCrbf.pickle\"), \"rb\")\n",
    "SVCrbf = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/SVCLinear.pickle\"), \"rb\")\n",
    "SVCLinear = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/KNN.pickle\"), \"rb\")\n",
    "KNN = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/Gradient_Boosting.pickle\"), \"rb\")\n",
    "Gradient_Boosting = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/AdaBoost_Ensemble.pickle\"), \"rb\")\n",
    "Ada_Boost_Ensemble = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/Bagger_ensemble.pickle\"), \"rb\")\n",
    "Bagger_ensemble = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/StochasticGradientDescent.pickle\"), \"rb\")\n",
    "StochasticGradientDescent = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/Gaussian.pickle\"), \"rb\")\n",
    "Gaussian = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "open_file = open(os.path.join(path,\"Pickle/PCA_SVC.pickle\"), \"rb\")\n",
    "PCA_SVC = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex  Age  Parch  Fare  Embarked  Name_length  Has_Cabin  \\\n",
       "304       3    1    1      0     1         0           33          0   \n",
       "149       2    1    2      0     1         0           33          0   \n",
       "484       1    1    1      0     3         1           23          1   \n",
       "430       1    1    1      0     2         0           41          1   \n",
       "722       2    1    2      0     1         0           28          0   \n",
       "\n",
       "     FamilySize  IsAlone  Title  \n",
       "304           1        1      1  \n",
       "149           1        1      5  \n",
       "484           2        0      1  \n",
       "430           1        1      1  \n",
       "722           1        1      1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = Logistic_Regression\n",
    "RF = Random_Forest\n",
    "SVMR = SVCrbf\n",
    "SVML = SVCLinear\n",
    "KNC = KNN\n",
    "GBC = Gradient_Boosting\n",
    "ABC = Ada_Boost_Ensemble\n",
    "BC = Bagger_ensemble\n",
    "GAU = Gaussian\n",
    "SGD = StochasticGradientDescent\n",
    "PCA_SVC_LI = PCA_SVC\n",
    "\n",
    "models= list(zip([LR,\n",
    "                  RF,\n",
    "                  SVMR,\n",
    "                  SVCLinear,\n",
    "                  KNC,\n",
    "                  GBC, \n",
    "                  ABC,\n",
    "                  BC,\n",
    "                  PCA_SVC_LI,\n",
    "                  GAU,\n",
    "                  SGD], \n",
    "                      ['Logistic Regression', \n",
    "                       'Random Forest', \n",
    "                       'RBG Support Vector Classifier',\n",
    "                       'Linear Support Vector Classifier'\n",
    "                       'KNeighbors',\n",
    "                       'Gradient Boosting',\n",
    "                       'Ada Boost',\n",
    "                       'Bagging',\n",
    "                       'Principle Component Linear SVC',\n",
    "                       'Gaussian',\n",
    "                       'Stochastic Gradient Descent']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.79 (+/- 0.02) [Logistic Regression]\n",
      "Test Accuracy: 0.81 \n",
      "Train CV Accuracy: 0.82 (+/- 0.01) [Random Forest]\n",
      "Test Accuracy: 0.87 \n",
      "Train CV Accuracy: 0.80 (+/- 0.01) [RBG Support Vector Classifier]\n",
      "Test Accuracy: 0.89 \n",
      "Train CV Accuracy: 0.80 (+/- 0.02) [Linear Support Vector ClassifierKNeighbors]\n",
      "Test Accuracy: 0.84 \n",
      "Train CV Accuracy: 0.76 (+/- 0.02) [Gradient Boosting]\n",
      "Test Accuracy: 0.99 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [Ada Boost]\n",
      "Test Accuracy: 0.87 \n",
      "Train CV Accuracy: 0.81 (+/- 0.02) [Bagging]\n",
      "Test Accuracy: 0.86 \n",
      "Train CV Accuracy: 0.78 (+/- 0.03) [Principle Component Linear SVC]\n",
      "Test Accuracy: 0.98 \n",
      "Train CV Accuracy: 0.81 (+/- 0.01) [Gaussian]\n",
      "Test Accuracy: 0.85 \n",
      "Train CV Accuracy: 0.78 (+/- 0.03) [Stochastic Gradient Descent]\n",
      "Test Accuracy: 0.80 \n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in models:\n",
    "    scores = cross_validation.cross_val_score(clf, X_train, y_train,cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n",
    "\n",
    "Hard- Mode\n",
    "Soft- Probabilistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.82 (+/- 0.01) [Ensemble Hard Voting]\n",
      "Test Accuracy: 0.88 \n",
      "Train CV Accuracy: 0.81 (+/- 0.03) [Ensemble Soft Voting]\n",
      "Test Accuracy: 0.94 \n"
     ]
    }
   ],
   "source": [
    "# ### Ensemble Voting\n",
    "print('5-fold cross validation:\\n')\n",
    "\n",
    "# [LR, RF, SVM, KNC,GBC,ABC, BC]\n",
    "\n",
    "allmodel = [LR, RF, SVMR, SVCLinear, KNC, GBC, ABC, BC, PCA_SVC_LI, GAU]\n",
    "best= [LR, RF, SVMR, SVCLinear, KNC, GBC, ABC, BC, GAU]\n",
    "\n",
    "ECH = EnsembleVoteClassifier(allmodel, voting='hard')\n",
    "ECS = EnsembleVoteClassifier(best, voting='soft', weights=[1,2,2,1,2,1,1,1,1])\n",
    "\n",
    "for clf, label in zip([ECH, ECS], \n",
    "                      ['Ensemble Hard Voting', \n",
    "                       'Ensemble Soft Voting']):\n",
    "    scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "418\n"
     ]
    }
   ],
   "source": [
    "ensembling(ECH,\"Hard_ensemble1\")\n",
    "ensembling(ECS,\"Soft_ensemble1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "Stacked generalized modesl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 11) (891,) (418, 11)\n",
      "5-fold cross validation:\n",
      "\n",
      "##### Base Model 0 #####\n",
      "Train CV Accuracy: 0.80 (+/- 0.02)\n",
      "Train Accuracy: 0.86 \n",
      "Test Accuracy: 0.88 \n",
      "##### Base Model 1 #####\n",
      "Train CV Accuracy: 0.80 (+/- 0.03)\n",
      "Train Accuracy: 0.95 \n",
      "Test Accuracy: 0.83 \n",
      "##### Base Model 2 #####\n",
      "Train CV Accuracy: 0.81 (+/- 0.02)\n",
      "Train Accuracy: 0.83 \n",
      "Test Accuracy: 0.87 \n",
      "##### Meta Model #####\n",
      "Train CV Accuracy: 0.96 (+/- 0.02)\n",
      "Train Accuracy: 0.96 \n",
      "Test Accuracy: 0.82 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "np.random.seed(2017)  # seed to shuffle the train set\n",
    "\n",
    "X = train_df.drop([\"Survived\"] , axis=1)\n",
    "y = train_df[\"Survived\"]\n",
    "\n",
    "oosample  = test_df.drop([\"PassengerId\"] , axis=1).copy()\n",
    "print(X.shape, y.shape, oosample.shape)\n",
    "\n",
    "#Normalize\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# evaluate the model by splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)\n",
    "\n",
    "kfold = cross_validation.StratifiedKFold(y=y_train, n_folds=5, random_state=2017)\n",
    "num_trees = 10\n",
    "verbose = True # to print the progress\n",
    "\n",
    "clfs = [KNeighborsClassifier(),\n",
    "        RandomForestClassifier(n_estimators=num_trees, random_state=2017),\n",
    "        GradientBoostingClassifier(n_estimators=num_trees, random_state=2017)]\n",
    "\n",
    "# Creating train and test sets for blending\n",
    "dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_test.shape[0], len(clfs)))\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "for i, clf in enumerate(clfs):   \n",
    "    scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    print(\"##### Base Model %0.0f #####\" % i)\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "    clf.fit(X_train, y_train)   \n",
    "    print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_train), y_train)))\n",
    "    dataset_blend_train[:,i] = clf.predict_proba(X_train)[:, 1]\n",
    "    dataset_blend_test[:,i] = clf.predict_proba(X_test)[:, 1]\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))    \n",
    "\n",
    "print(\"##### Meta Model #####\")\n",
    "clf = LogisticRegression()\n",
    "scores = cross_validation.cross_val_score(clf, dataset_blend_train, y_train, cv=kfold, scoring='accuracy')\n",
    "clf.fit(dataset_blend_train, y_train)\n",
    "print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_train), y_train)))\n",
    "print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflection on model\n",
    "\n",
    "Ensemble models suggest that testing accuracy is in the high 80s, however when applied to Kaggle out of sample data, perfromance is consistently in the high 70s. This suggests that either the submission data is very different, or that my model is overfitting on the given data.\n",
    "\n",
    " - May want to explore models with a greater emphasis on randomness in order to tone down the overfitting.\n",
    " - Perhaps compare variable distribution between submission data and training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
